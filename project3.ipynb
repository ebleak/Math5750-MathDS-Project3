{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Math 5750/6880: Mathematics of Data Science \\\n",
        "Project 3"
      ],
      "metadata": {
        "id": "0gdC70xxFyc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fashion-MNIST image classification using sklearn"
      ],
      "metadata": {
        "id": "i9_7SnpMGKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "SmWUvOCl4d3p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(len(X_train), -1)\n",
        "X_test  = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "AB136H0PGKq1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "#For this command I asked chatgpt to give me an explination of what each parameter of the function MLPClassifier does to better understand how to modify them\n",
        "\n",
        "#Baseline model with default parameters\n",
        "mlp_baseline = MLPClassifier(hidden_layer_sizes=(128,),\n",
        "                             activation='relu',\n",
        "                             solver='adam',\n",
        "                             learning_rate_init=0.001,\n",
        "                             early_stopping=False,\n",
        "                             alpha=0.0001,\n",
        "                             max_iter=20,\n",
        "                             random_state=42,\n",
        "                             verbose=False)\n",
        "\n",
        "start_time = time.time()\n",
        "mlp_baseline.fit(X_train, y_train)\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "y_pred = mlp_baseline.predict(X_test)\n",
        "\n",
        "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(f\"Baseline Training Time: {train_time:.2f} s\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "5GAsN-dmHjRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4ec951-8cae-4d40-ae5a-d34404011de6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.886\n",
            "Baseline Training Time: 33.52 s\n",
            "[[823   2  22  21   4   2 120   0   6   0]\n",
            " [  2 980   2   9   4   0   2   0   1   0]\n",
            " [ 19   0 847   9  63   1  59   0   2   0]\n",
            " [ 18   8  22 879  48   0  23   0   2   0]\n",
            " [  0   1 119  21 807   0  50   0   2   0]\n",
            " [  0   0   0   1   0 961   0  16   3  19]\n",
            " [102   2 106  20  70   0 691   0   9   0]\n",
            " [  0   0   0   0   0  19   0 954   0  27]\n",
            " [  7   0   7   4   5   6   8   3 959   1]\n",
            " [  1   0   0   0   0   9   1  30   0 959]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcation to iterate over different conditions with the baseline as default\n",
        "def train_mlp(X_train, y_train, X_test, y_test,\n",
        "              layers=(128,),\n",
        "              activation='relu',\n",
        "              solver='adam',\n",
        "              learning=0.001,\n",
        "              estop=False,\n",
        "              alpha=0.0001,\n",
        "              max_iter=20, #Do not change\n",
        "              random_state=42, #Do not change\n",
        "              verbose=False): #Do not change\n",
        "\n",
        "    #Define Model\n",
        "    model = MLPClassifier(hidden_layer_sizes=layers,\n",
        "                          activation=activation,\n",
        "                          solver=solver,\n",
        "                          learning_rate_init=learning,\n",
        "                          early_stopping=estop,\n",
        "                          alpha=alpha,\n",
        "                          max_iter=max_iter,\n",
        "                          random_state=random_state,\n",
        "                          verbose=verbose)\n",
        "\n",
        "    #Train\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    #Evaluation Metrics\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    conf_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f} | Training Time: {train_time:.2f} s\")\n",
        "    print(conf_mat)\n",
        "\n",
        "    return acc, conf_mat"
      ],
      "metadata": {
        "id": "hSK2aswvzkj1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Different Conditions\n",
        "params = [\n",
        "    ((128,), 'relu', 'adam', 0.001, False), #1 hidden layer neuron count test\n",
        "    ((256,), 'relu', 'adam', 0.001, False), #1 hidden layer neuron count test\n",
        "    ((256,128), 'relu', 'adam', 0.001, False), #2 hidden layers test\n",
        "    ((256,128,64), 'relu', 'adam', 0.001, False), #3 hidden layers test\n",
        "    ((128,), 'tanh', 'adam', 0.001, False), #Activation test\n",
        "    ((128,), 'relu', 'sgd', 0.001, False), #Solver test\n",
        "    ((128,), 'relu', 'adam', 0.01, False), #Large learning rate test\n",
        "    ((128,), 'relu', 'adam', 0.0001, False), #Small learning rate test\n",
        "    ((128,), 'relu', 'adam', 0.001, True) #Early stopping test\n",
        "]\n",
        "\n",
        "#Iterate over parameters\n",
        "results = []\n",
        "for i, (a,b,c,d,e) in enumerate(params, 1):\n",
        "    print(f\"\\n--- Training Model {i} ---\")\n",
        "    acc, conf_mat = train_mlp(X_train, y_train, X_test, y_test, a, b, c, d, e)\n",
        "    results.append((i, acc, conf_mat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJvbf4UrzyGP",
        "outputId": "97a061c5-09cb-4340-b027-a37489780be1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Model 1 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8860 | Training Time: 68.15 s\n",
            "[[823   2  22  21   4   2 120   0   6   0]\n",
            " [  2 980   2   9   4   0   2   0   1   0]\n",
            " [ 19   0 847   9  63   1  59   0   2   0]\n",
            " [ 18   8  22 879  48   0  23   0   2   0]\n",
            " [  0   1 119  21 807   0  50   0   2   0]\n",
            " [  0   0   0   1   0 961   0  16   3  19]\n",
            " [102   2 106  20  70   0 691   0   9   0]\n",
            " [  0   0   0   0   0  19   0 954   0  27]\n",
            " [  7   0   7   4   5   6   8   3 959   1]\n",
            " [  1   0   0   0   0   9   1  30   0 959]]\n",
            "\n",
            "--- Training Model 2 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8898 | Training Time: 101.83 s\n",
            "[[849   3  15  22   6   1  95   0   9   0]\n",
            " [  5 977   2  10   5   0   1   0   0   0]\n",
            " [ 22   1 825   8  87   0  55   0   2   0]\n",
            " [ 18   8  21 871  63   0  14   0   5   0]\n",
            " [  0   1  95  16 844   0  41   0   3   0]\n",
            " [  0   0   0   1   0 960   0  24   1  14]\n",
            " [104   2  91  26  74   0 695   0   8   0]\n",
            " [  0   0   0   0   0   8   0 955   0  37]\n",
            " [ 10   1  10   5   4   6   5   4 955   0]\n",
            " [  1   0   0   1   0   7   1  23   0 967]]\n",
            "\n",
            "--- Training Model 3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8901 | Training Time: 128.93 s\n",
            "[[876   1  15  13   2   1  85   0   7   0]\n",
            " [  6 979   3   6   1   0   4   0   1   0]\n",
            " [ 23   0 846   7  59   0  64   0   1   0]\n",
            " [ 40   9  12 893  21   1  20   0   4   0]\n",
            " [  3   0 104  36 796   0  59   0   2   0]\n",
            " [  1   0   0   0   0 959   0  27   0  13]\n",
            " [157   1  84  25  60   0 667   0   6   0]\n",
            " [  0   0   0   0   0   9   0 977   0  14]\n",
            " [  3   0   6   3   5   3   9   6 965   0]\n",
            " [  0   0   0   0   0  11   1  45   0 943]]\n",
            "\n",
            "--- Training Model 4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8863 | Training Time: 143.37 s\n",
            "[[824   0  16  22   4   1 127   0   6   0]\n",
            " [  4 979   1  11   3   0   2   0   0   0]\n",
            " [ 19   0 847   7  50   1  72   0   4   0]\n",
            " [ 22   4  17 876  40   2  31   0   8   0]\n",
            " [  0   1 114  24 803   0  54   0   4   0]\n",
            " [  1   0   1   1   0 949   0  23   0  25]\n",
            " [134   0  88  20  54   0 696   0   8   0]\n",
            " [  0   0   0   0   0  12   0 956   1  31]\n",
            " [  4   0   3   4   3   4   9   4 969   0]\n",
            " [  0   0   0   0   0   3   1  32   0 964]]\n",
            "\n",
            "--- Training Model 5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8837 | Training Time: 74.87 s\n",
            "[[798   3  15  30   7   0 138   0   9   0]\n",
            " [  4 968   0  17   5   0   3   0   3   0]\n",
            " [ 14   1 817  14  85   2  62   1   4   0]\n",
            " [ 14   8  15 893  34   1  31   0   4   0]\n",
            " [  2   1  95  22 833   1  42   0   4   0]\n",
            " [  0   0   0   1   0 951   0  24   4  20]\n",
            " [ 90   2  83  28  76   0 714   0   6   1]\n",
            " [  0   0   0   0   0  23   0 949   0  28]\n",
            " [  4   1   7   8   6   4  13   5 952   0]\n",
            " [  0   0   0   0   0  12   1  25   0 962]]\n",
            "\n",
            "--- Training Model 6 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8646 | Training Time: 51.53 s\n",
            "[[812   1  11  52   2   0 110   0  11   1]\n",
            " [  1 958   4  27   4   0   5   0   1   0]\n",
            " [ 16   1 783  16 105   1  73   0   5   0]\n",
            " [ 21  10  11 886  34   1  32   0   5   0]\n",
            " [  0   1  95  34 797   0  69   0   4   0]\n",
            " [  0   0   0   0   0 928   0  46   2  24]\n",
            " [118   2  95  42  85   1 647   0  10   0]\n",
            " [  0   0   0   0   0  30   0 939   0  31]\n",
            " [  1   1   6  10   4   6  19   4 949   0]\n",
            " [  0   0   0   1   0  15   1  36   0 947]]\n",
            "\n",
            "--- Training Model 7 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8671 | Training Time: 58.92 s\n",
            "[[827   3   9  28   2   0 120   0  11   0]\n",
            " [  5 975   0  11   4   0   4   0   1   0]\n",
            " [ 49   1 776  19  81   1  72   0   1   0]\n",
            " [ 34  14  18 862  35   0  31   0   6   0]\n",
            " [  9   0 108  30 791   1  57   0   4   0]\n",
            " [  0   0   0   1   0 905   0  54  24  16]\n",
            " [121   1  75  33  96   1 661   0  12   0]\n",
            " [  0   0   0   0   0   5   0 962   1  32]\n",
            " [  5   1   0   6   2   2   8   3 973   0]\n",
            " [  0   0   0   0   0  10   1  50   0 939]]\n",
            "\n",
            "--- Training Model 8 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8781 | Training Time: 60.04 s\n",
            "[[830   0  11  33   2   0 114   0   9   1]\n",
            " [  2 963   2  23   3   0   5   0   2   0]\n",
            " [ 13   0 795  15 101   1  72   0   3   0]\n",
            " [ 16   6  12 897  33   1  31   0   4   0]\n",
            " [  0   0  89  31 810   0  68   0   2   0]\n",
            " [  0   0   0   1   0 939   0  39   3  18]\n",
            " [114   1  83  34  77   0 681   0  10   0]\n",
            " [  0   0   0   0   0  22   0 955   0  23]\n",
            " [  3   1   3   8   4   5   9   4 962   1]\n",
            " [  0   0   0   1   0   8   1  41   0 949]]\n",
            "\n",
            "--- Training Model 9 ---\n",
            "Accuracy: 0.8860 | Training Time: 55.41 s\n",
            "[[853   2   8  14   4   0 109   0  10   0]\n",
            " [  4 978   2   8   4   0   3   0   1   0]\n",
            " [ 27   1 818  11  67   1  72   1   2   0]\n",
            " [ 29   8   8 875  44   1  31   0   4   0]\n",
            " [  1   1 107  18 803   0  69   0   1   0]\n",
            " [  0   0   0   1   0 954   0  27   2  16]\n",
            " [128   0  74  21  68   2 700   0   7   0]\n",
            " [  0   0   0   0   0  16   0 961   0  23]\n",
            " [  8   0   5   3   3   3   6   5 966   1]\n",
            " [  0   0   0   1   0  11   2  34   0 952]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attempt to run with optimal conditions\n",
        "params = [\n",
        "    ((256,128), 'relu', 'adam', 0.001, True)\n",
        "]\n",
        "\n",
        "results = []\n",
        "for i, (a,b,c,d,e) in enumerate(params, 1):\n",
        "    print(f\"\\n--- Training Model {i} ---\")\n",
        "    acc, conf_mat = train_mlp(X_train, y_train, X_test, y_test, a, b, c, d, e)\n",
        "    results.append((i, acc, conf_mat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ2s7FqWH5p4",
        "outputId": "33de5e3c-27e9-4ae5-d3cd-e2578db876e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Model 1 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8894 | Training Time: 121.08 s\n",
            "[[837   2  19  21   4   0 110   0   6   1]\n",
            " [  3 978   2  13   1   0   2   0   1   0]\n",
            " [ 15   1 837  14  76   1  56   0   0   0]\n",
            " [ 16   6   8 911  29   0  29   0   1   0]\n",
            " [  1   0 101  30 813   0  54   0   1   0]\n",
            " [  0   0   0   0   0 954   0  27   2  17]\n",
            " [125   1  91  28  73   0 676   0   6   0]\n",
            " [  0   0   0   0   0   8   0 958   1  33]\n",
            " [  6   1   6   9   2   6   7   3 960   0]\n",
            " [  0   0   0   0   0   9   1  20   0 970]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fashion-MNIST image classification  using pytorch"
      ],
      "metadata": {
        "id": "a2qcKggmIH8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# scale to [0,1], add channel dimension -> (N, 1, 28, 28)\n",
        "X_train = (X_train.astype(\"float32\") / 255.0)[:, None, :, :]\n",
        "X_test  = (X_test.astype(\"float32\")  / 255.0)[:,  None, :, :]\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "# train/val split: last 10k of train as validation\n",
        "X_tr, X_val = X_train[:50000], X_train[50000:]\n",
        "y_tr, y_val = y_train[:50000], y_train[50000:]\n",
        "\n",
        "# wrap in PyTorch TensorDatasets and DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(X_tr),  torch.from_numpy(y_tr))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "B9IQwhgcIVOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acfae53-b955-4f62-f16c-01cf84334724"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "# In colab, you should ``change runtime type'' to GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#Baseline model\n",
        "class BaseNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_fc = BaseNN().to(device)\n",
        "summary(model_fc, (1, 28, 28))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_fc.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, loader, criterion, optimizer, epochs=5):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(loader):.4f}\")\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Training completed in {elapsed_time:.2f} seconds\\n\")\n",
        "\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {correct / total:.2f}\")\n",
        "\n",
        "train(model_fc, train_loader, criterion, optimizer, epochs=5)\n",
        "test(model_fc, test_loader)"
      ],
      "metadata": {
        "id": "EjuGfbpeQDpM",
        "outputId": "c3f82267-3dcf-40ce-c86e-466413f73cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 256]         200,960\n",
            "              ReLU-3                  [-1, 256]               0\n",
            "            Linear-4                  [-1, 128]          32,896\n",
            "              ReLU-5                  [-1, 128]               0\n",
            "            Linear-6                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 235,146\n",
            "Trainable params: 235,146\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.90\n",
            "Estimated Total Size (MB): 0.91\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/5, Loss: 0.5937\n",
            "Epoch 2/5, Loss: 0.4023\n",
            "Epoch 3/5, Loss: 0.3577\n",
            "Epoch 4/5, Loss: 0.3286\n",
            "Epoch 5/5, Loss: 0.3072\n",
            "Training completed in 7.73 seconds\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Variations\n",
        "\n",
        "#Model with more neurons\n",
        "print(\"--- More Neurons ---\")\n",
        "class BiggerNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BiggerNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model_big = BiggerNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_big.parameters(), lr=0.001)\n",
        "\n",
        "train(model_big, train_loader, criterion, optimizer, epochs=5)\n",
        "test(model_big, test_loader)\n",
        "\n",
        "#Model with tanh activation\n",
        "print(\"--- Activation Method ---\")\n",
        "class TanhNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TanhNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_tanh = TanhNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_tanh.parameters(), lr=0.001)\n",
        "\n",
        "train(model_tanh, train_loader, criterion, optimizer, epochs=5)\n",
        "test(model_tanh, test_loader)\n",
        "\n",
        "#Optimizer and Learning Rate\n",
        "print(\"--- Optimizer and Learning Rate ---\")\n",
        "model_sgd = BaseNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_sgd.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "train(model_sgd, train_loader, criterion, optimizer, epochs=5)\n",
        "test(model_sgd, test_loader)\n",
        "\n",
        "#Early Stopping and Regulaization - This variation was written using the help of generative AI beacsue I could not figure out an equivlant to this intially\n",
        "print(\"--- Early Stopping and Regularization ---\")\n",
        "class DropoutNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.drop2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_drop = DropoutNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_drop.parameters(), lr=0.001)\n",
        "\n",
        "best_acc = 0\n",
        "patience, counter = 2, 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model_drop, train_loader, criterion, optimizer, epochs=1)\n",
        "    model_drop.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model_drop(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"Validation Accuracy: {acc:.2f}%\")\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break"
      ],
      "metadata": {
        "id": "1fOuhbLuljAn",
        "outputId": "e639780c-7b46-4a23-ae69-76f235c42a93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- More Neurons ---\n",
            "Epoch 1/5, Loss: 0.5850\n",
            "Epoch 2/5, Loss: 0.3855\n",
            "Epoch 3/5, Loss: 0.3375\n",
            "Epoch 4/5, Loss: 0.3144\n",
            "Epoch 5/5, Loss: 0.2926\n",
            "Training completed in 5.69 seconds\n",
            "\n",
            "Test Accuracy: 0.88\n",
            "--- Activation Method ---\n",
            "Epoch 1/5, Loss: 0.5437\n",
            "Epoch 2/5, Loss: 0.3869\n",
            "Epoch 3/5, Loss: 0.3481\n",
            "Epoch 4/5, Loss: 0.3246\n",
            "Epoch 5/5, Loss: 0.3082\n",
            "Training completed in 5.53 seconds\n",
            "\n",
            "Test Accuracy: 0.87\n",
            "--- Optimizer and Learning Rate ---\n",
            "Epoch 1/5, Loss: 0.6374\n",
            "Epoch 2/5, Loss: 0.4050\n",
            "Epoch 3/5, Loss: 0.3627\n",
            "Epoch 4/5, Loss: 0.3401\n",
            "Epoch 5/5, Loss: 0.3194\n",
            "Training completed in 4.56 seconds\n",
            "\n",
            "Test Accuracy: 0.86\n",
            "--- Early Stopping and Regularization ---\n",
            "Epoch 1/1, Loss: 0.6691\n",
            "Training completed in 1.07 seconds\n",
            "\n",
            "Validation Accuracy: 83.05%\n",
            "Epoch 1/1, Loss: 0.4411\n",
            "Training completed in 1.15 seconds\n",
            "\n",
            "Validation Accuracy: 85.05%\n",
            "Epoch 1/1, Loss: 0.4004\n",
            "Training completed in 1.37 seconds\n",
            "\n",
            "Validation Accuracy: 85.35%\n",
            "Epoch 1/1, Loss: 0.3765\n",
            "Training completed in 1.17 seconds\n",
            "\n",
            "Validation Accuracy: 86.41%\n",
            "Epoch 1/1, Loss: 0.3580\n",
            "Training completed in 1.11 seconds\n",
            "\n",
            "Validation Accuracy: 86.47%\n",
            "Epoch 1/1, Loss: 0.3452\n",
            "Training completed in 1.11 seconds\n",
            "\n",
            "Validation Accuracy: 87.07%\n",
            "Epoch 1/1, Loss: 0.3353\n",
            "Training completed in 1.09 seconds\n",
            "\n",
            "Validation Accuracy: 87.02%\n",
            "Epoch 1/1, Loss: 0.3204\n",
            "Training completed in 1.09 seconds\n",
            "\n",
            "Validation Accuracy: 87.19%\n",
            "Epoch 1/1, Loss: 0.3147\n",
            "Training completed in 1.10 seconds\n",
            "\n",
            "Validation Accuracy: 87.67%\n",
            "Epoch 1/1, Loss: 0.3036\n",
            "Training completed in 1.09 seconds\n",
            "\n",
            "Validation Accuracy: 87.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = CNNModel().to(device)\n",
        "summary(model_cnn, (1, 28, 28))\n",
        "\n",
        "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
        "train(model_cnn, train_loader, criterion, optimizer, epochs=5)\n",
        "test(model_cnn, test_loader)\n"
      ],
      "metadata": {
        "id": "FpFM7khyQPDF",
        "outputId": "881cac0c-50c6-499a-873d-f14bb8bd1858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "              ReLU-2           [-1, 32, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
            "            Conv2d-4           [-1, 64, 14, 14]          18,496\n",
            "              ReLU-5           [-1, 64, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
            "           Flatten-7                 [-1, 3136]               0\n",
            "            Linear-8                  [-1, 128]         401,536\n",
            "              ReLU-9                  [-1, 128]               0\n",
            "           Linear-10                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 421,642\n",
            "Trainable params: 421,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.67\n",
            "Params size (MB): 1.61\n",
            "Estimated Total Size (MB): 2.28\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/5, Loss: 0.5643\n",
            "Epoch 2/5, Loss: 0.3363\n",
            "Epoch 3/5, Loss: 0.2863\n",
            "Epoch 4/5, Loss: 0.2556\n",
            "Epoch 5/5, Loss: 0.2367\n",
            "Training completed in 8.98 seconds\n",
            "\n",
            "Test Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transfer Learning\n",
        "resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, 10)\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=1e-4)\n",
        "\n",
        "train(resnet, train_loader, criterion, optimizer, epochs=5)\n",
        "test(resnet, test_loader)\n"
      ],
      "metadata": {
        "id": "o4TOGiAJQQIw",
        "outputId": "ac5a500e-35b1-4de6-d895-d3f2cd8cf844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.5530\n",
            "Epoch 2/5, Loss: 0.3412\n",
            "Epoch 3/5, Loss: 0.2843\n",
            "Epoch 4/5, Loss: 0.2420\n",
            "Epoch 5/5, Loss: 0.2120\n",
            "Training completed in 100.02 seconds\n",
            "\n",
            "Test Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CajNc83rQSlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}